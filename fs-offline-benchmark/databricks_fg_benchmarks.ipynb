{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a470a6d-c52f-414c-bd61-c6c6cda16347",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks-feature-store && pip install duckdb --pre --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0d2a8e5-b561-4eb8-9b04-3dda6e15f9bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from databricks.feature_store import FeatureLookup\n",
    "\n",
    "duckdb.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2303c72-7692-43fb-b5f6-ac182e6b3a52",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0876def-1a03-4653-a6c1-22d382d233c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_store import FeatureStoreClient\n",
    "fs = FeatureStoreClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e8d3451-9a76-4142-ae0b-cbd8a53e0b28",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "MAX_MEMORY = \"40GB\" # increase to available python memory -25%\n",
    "TMP_DIR = \"fg-data-v8\"\n",
    "DUCKDB_FILE = f\"{TMP_DIR}/taxi.duckdb\"\n",
    "DATA_FOLDER = f\"{TMP_DIR}/taxidata\" \n",
    "\n",
    "# S3 Uploads\n",
    "AWS_ACCESS_KEY=''\n",
    "AWS_SECRET_ACCESS_KEY=''\n",
    "AWS_REGION='us-east-2'\n",
    "BUCKET = \"hopsworks-bench-datasets\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "607cf2c9-3778-4162-864e-8845c19310df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!mkdir -p {TMP_DIR}\n",
    "!mkdir -p {DATA_FOLDER}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "113be4a1-9ef7-4097-8195-2a1454dc3e04",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "con = duckdb.connect(DUCKDB_FILE, config={'memory_limit': MAX_MEMORY, 'temp_directory': TMP_DIR}) \n",
    "con.execute(\"INSTALL httpfs;\")\n",
    "con.execute(\"INSTALL parquet;\")\n",
    "con.execute(\"LOAD httpfs;\")\n",
    "con.execute(\"LOAD parquet;\")\n",
    "con.execute(f\"\"\"\n",
    "    SET s3_region='{AWS_REGION}';\n",
    "    SET s3_access_key_id='{AWS_ACCESS_KEY}';\n",
    "    SET s3_secret_access_key='{AWS_SECRET_ACCESS_KEY}';\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bba6150-8308-4a40-9fc4-9005f4083ffd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!ls -l {TMP_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f82c2ea-655f-40fc-b705-f97be35ae87c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_raw_data(sf):\n",
    "    query = f'''\n",
    "      SELECT \n",
    "          tpep_pickup_datetime,\n",
    "          trip_distance,\n",
    "          tip_amount,\n",
    "          fare_amount\n",
    "      FROM \n",
    "          read_parquet([\n",
    "            's3://{BUCKET}/taxidata_cleaned/2011.parquet', \n",
    "            's3://{BUCKET}/taxidata_cleaned/2012.parquet', \n",
    "            's3://{BUCKET}/taxidata_cleaned/2013.parquet',\n",
    "            's3://{BUCKET}/taxidata_cleaned/2014.parquet',\n",
    "            's3://{BUCKET}/taxidata_cleaned/2015.parquet',\n",
    "            's3://{BUCKET}/taxidata_cleaned/2016.parquet'\n",
    "          ])\n",
    "      LIMIT {sf*1000000}\n",
    "    '''\n",
    "    raw_data = con.execute(query).df()\n",
    "    # Add row_id index to raw_data\n",
    "    raw_data['row_id'] = raw_data.reset_index().index\n",
    "    row_id = raw_data.pop('row_id')\n",
    "    raw_data.insert(0, 'row_id', row_id)\n",
    "    return raw_data\n",
    "  \n",
    "def read_feature_data(limit, offset):\n",
    "    lim = limit\n",
    "    off = offset\n",
    "    query = f'''\n",
    "      CREATE \n",
    "      OR REPLACE \n",
    "      VIEW taxidata \n",
    "      AS\n",
    "      SELECT \n",
    "          tpep_pickup_datetime, \n",
    "          pu_location_id, \n",
    "          pu_borough,\n",
    "          pu_svc_zone,\n",
    "          pu_zone \n",
    "      FROM \n",
    "          read_parquet([\n",
    "            's3://{BUCKET}/taxidata_cleaned/2011.parquet', \n",
    "            's3://{BUCKET}/taxidata_cleaned/2012.parquet', \n",
    "            's3://{BUCKET}/taxidata_cleaned/2013.parquet',\n",
    "            's3://{BUCKET}/taxidata_cleaned/2014.parquet',\n",
    "            's3://{BUCKET}/taxidata_cleaned/2015.parquet',\n",
    "            's3://{BUCKET}/taxidata_cleaned/2016.parquet'\n",
    "          ])\n",
    "    '''\n",
    "    con.execute(query)\n",
    "    raw_data = con.execute(f\"SELECT * FROM taxidata LIMIT {lim} OFFSET {off}\").df()\n",
    "    # Add row_id index to raw_data\n",
    "    raw_data['row_id'] = raw_data.reset_index().index\n",
    "    row_id = raw_data.pop('row_id')\n",
    "    raw_data.insert(0, 'row_id', row_id)\n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "864d7d57-bace-4439-a9fe-8f86c1b5a8e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql CREATE DATABASE IF NOT EXISTS feature_store_taxi_example;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a5091f3-3eec-4c23-8dae-79ff8d0bef16",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\",\"true\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.fallback.enabled\",\"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58d837ed-bfee-4327-9eab-1c2876b58fa5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scale_factor = [5,10,20,50,100]\n",
    "limit = 10000000 # Get 10M at a time so it's faster\n",
    "\n",
    "for sf in scale_factor:\n",
    "    offset = 0\n",
    "    total_rows = sf * 1000000  # Millions\n",
    "    while offset < total_rows:\n",
    "      # Get feature data\n",
    "      pickup_features = read_feature_data(limit, offset)\n",
    "      print(f\"Total rows: {total_rows}; Offset: {offset}\")\n",
    "      # Conver to Spark DF\n",
    "      sparkDF=spark.createDataFrame(pickup_features)\n",
    "      # Create table for first update\n",
    "      if offset == 0:\n",
    "          fs.create_table(\n",
    "            name=f\"feature_store_taxi_example.pickup_features_{sf}\",\n",
    "            primary_keys=[\"row_id\"],\n",
    "            timestamp_keys=[\"tpep_pickup_datetime\"],\n",
    "            df=sparkDF,\n",
    "            description=\"NYC Taxi data pickup features\",\n",
    "          )\n",
    "      else:\n",
    "        # Merge insert for offset > 0  \n",
    "        fs.write_table(\n",
    "          name=f\"feature_store_taxi_example.pickup_features_{sf}\",\n",
    "          df=sparkDF,\n",
    "          mode=\"merge\",\n",
    "        )\n",
    "      offset += limit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31d9d020-5d46-40a9-877e-93e8e610541b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Benchmarks FG Reads"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c62bf167-d6ba-4446-8beb-eb8a1bf97b8e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# SF = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdb6c081-41a8-4d1b-9dfd-db929ebc4326",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sf = 5\n",
    "pickup_feature_lookups = [\n",
    "    FeatureLookup(\n",
    "        table_name=f\"feature_store_taxi_example.pickup_features_{sf}\",\n",
    "        feature_names=[\n",
    "            \"pu_location_id\",\n",
    "            \"pu_borough\",\n",
    "            \"pu_svc_zone\",\n",
    "            \"pu_zone\",\n",
    "        ],\n",
    "        lookup_key=[\"row_id\"],\n",
    "        timestamp_lookup_key=\"tpep_pickup_datetime\",\n",
    "    ),\n",
    "]\n",
    "raw_data = get_raw_data(sf)\n",
    "spark_raw_data = spark.createDataFrame(raw_data)\n",
    "\n",
    "start = time.time()\n",
    "training_set = fs.create_training_set(\n",
    "    spark_raw_data,\n",
    "    feature_lookups=pickup_feature_lookups,\n",
    "    label=\"fare_amount\",\n",
    ")\n",
    "training_df = training_set.load_df()\n",
    "pdf = training_df.write.mode(\"overwrite\").parquet(f\"training-{sf}.parquet\")\n",
    "# toParquet\n",
    "print(f\"time for SF {sf} write to parquet: {time.time() - start}\")\n",
    "print(f\"Num of rows of training data: {training_df.count()}\")\n",
    "\n",
    "# Convert to Pandas and count\n",
    "start = time.time()\n",
    "training_set = fs.create_training_set(\n",
    "    spark_raw_data,\n",
    "    feature_lookups=pickup_feature_lookups,\n",
    "    label=\"fare_amount\",\n",
    ")\n",
    "training_df = training_set.load_df()\n",
    "training_df.collect()\n",
    "pdf = training_df.toPandas()\n",
    "print(f\"time for SF {sf} collect pandas dataframe: {time.time() - start}\")\n",
    "print(f\"Num of rows of training data: {training_df.count()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4673b9bf-7905-4d42-9375-ba924318c74e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# SF = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fac0418f-5fa6-480f-a440-5a410765d2ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sf = 10\n",
    "pickup_feature_lookups = [\n",
    "    FeatureLookup(\n",
    "        table_name=f\"feature_store_taxi_example.pickup_features_{sf}\",\n",
    "        feature_names=[\n",
    "            \"pu_location_id\",\n",
    "            \"pu_borough\",\n",
    "            \"pu_svc_zone\",\n",
    "            \"pu_zone\",\n",
    "        ],\n",
    "        lookup_key=[\"row_id\"],\n",
    "        timestamp_lookup_key=\"tpep_pickup_datetime\",\n",
    "    ),\n",
    "]\n",
    "raw_data = get_raw_data(sf)\n",
    "spark_raw_data = spark.createDataFrame(raw_data)\n",
    "\n",
    "start = time.time()\n",
    "training_set = fs.create_training_set(\n",
    "    spark_raw_data,\n",
    "    feature_lookups=pickup_feature_lookups,\n",
    "    label=\"fare_amount\",\n",
    ")\n",
    "training_df = training_set.load_df()\n",
    "pdf = training_df.write.mode(\"overwrite\").parquet(f\"training-{sf}.parquet\")\n",
    "print(f\"time for SF {sf} write to parquet: {time.time() - start}\")\n",
    "print(f\"Num of rows of training data: {training_df.count()}\")\n",
    "\n",
    "# Convert to Pandas and count\n",
    "start = time.time()\n",
    "training_set = fs.create_training_set(\n",
    "    spark_raw_data,\n",
    "    feature_lookups=pickup_feature_lookups,\n",
    "    label=\"fare_amount\",\n",
    ")\n",
    "training_df = training_set.load_df()\n",
    "training_df.collect()\n",
    "pdf = training_df.toPandas()\n",
    "print(f\"time for SF {sf} collect pandas dataframe: {time.time() - start}\")\n",
    "print(f\"Num of rows of training data: {training_df.count()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcacfc30-e1ae-47cb-b828-09be024aebc3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# SF = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d6232af-f299-420e-9866-6dc00fe5bffd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sf = 20\n",
    "pickup_feature_lookups = [\n",
    "    FeatureLookup(\n",
    "        table_name=f\"feature_store_taxi_example.pickup_features_{sf}\",\n",
    "        feature_names=[\n",
    "            \"pu_location_id\",\n",
    "            \"pu_borough\",\n",
    "            \"pu_svc_zone\",\n",
    "            \"pu_zone\",\n",
    "        ],\n",
    "        lookup_key=[\"row_id\"],\n",
    "        timestamp_lookup_key=\"tpep_pickup_datetime\",\n",
    "    ),\n",
    "]\n",
    "raw_data = get_raw_data(sf)\n",
    "spark_raw_data = spark.createDataFrame(raw_data)\n",
    "spark_raw_data\n",
    "\n",
    "# Write to parquet\n",
    "start = time.time()\n",
    "training_set = fs.create_training_set(\n",
    "    spark_raw_data,\n",
    "    feature_lookups=pickup_feature_lookups,\n",
    "    label=\"fare_amount\",\n",
    ")\n",
    "training_df = training_set.load_df()\n",
    "pdf = training_df.toPandas()\n",
    "pdf = training_df.write.mode(\"overwrite\").parquet(f\"training-{sf}.parquet\")\n",
    "print(f\"time for SF {sf} write to parquet: {time.time() - start}\")\n",
    "print(f\"Num of rows of training data: {training_df.count()}\")\n",
    "\n",
    "# Convert to Pandas and count\n",
    "start = time.time()\n",
    "training_set = fs.create_training_set(\n",
    "    spark_raw_data,\n",
    "    feature_lookups=pickup_feature_lookups,\n",
    "    label=\"fare_amount\",\n",
    ")\n",
    "training_df = training_set.load_df()\n",
    "training_df.collect()\n",
    "pdf = training_df.toPandas()\n",
    "print(f\"time for SF {sf} collect pandas dataframe: {time.time() - start}\")\n",
    "print(f\"Num of rows of training data: {training_df.count()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ebd8d3f-bf86-4af6-8334-b151f460f481",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# SF = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09709623-5787-4bf4-a588-10e07e15da6a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sf = 50\n",
    "pickup_feature_lookups = [\n",
    "    FeatureLookup(\n",
    "        table_name=f\"feature_store_taxi_example.pickup_features_{sf}\",\n",
    "        feature_names=[\n",
    "            \"pu_location_id\",\n",
    "            \"pu_borough\",\n",
    "            \"pu_svc_zone\",\n",
    "            \"pu_zone\",\n",
    "        ],\n",
    "        lookup_key=[\"row_id\"],\n",
    "        timestamp_lookup_key=\"tpep_pickup_datetime\",\n",
    "    ),\n",
    "]\n",
    "raw_data = get_raw_data(sf)\n",
    "spark_raw_data = spark.createDataFrame(raw_data)\n",
    "\n",
    "spark_raw_data.cache()\n",
    "\n",
    "# Write to parquet\n",
    "start = time.time()\n",
    "training_set = fs.create_training_set(\n",
    "    spark_raw_data,\n",
    "    feature_lookups=pickup_feature_lookups,\n",
    "    label=\"fare_amount\",\n",
    ")\n",
    "training_df = training_set.load_df()\n",
    "pdf = training_df.toPandas()\n",
    "pdf = training_df.write.mode(\"overwrite\").parquet(f\"training-{sf}.parquet\")\n",
    "print(f\"time for SF {sf} write to parquet: {time.time() - start}\")\n",
    "print(f\"Num of rows of training data: {training_df.count()}\")\n",
    "\n",
    "# Convert to Pandas and count\n",
    "start = time.time()\n",
    "training_set = fs.create_training_set(\n",
    "    spark_raw_data,\n",
    "    feature_lookups=pickup_feature_lookups,\n",
    "    label=\"fare_amount\",\n",
    ")\n",
    "training_df = training_set.load_df()\n",
    "training_df.collect()\n",
    "pdf = training_df.toPandas()\n",
    "print(f\"time for SF {sf} collect pandas dataframe: {time.time() - start}\")\n",
    "print(f\"Num of rows of training data: {training_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69298d15-3673-490b-8f4a-63fda3431ad6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 679876467659874,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "databricks_fs_fg_benchmarks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
