{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfb46d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install --pre pandas==2.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fc3b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import hopsworks\n",
    "import time\n",
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "# duckdb.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c07ac33",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0934ffac",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_MEMORY = \"35GB\" # increase to available python memory -25%\n",
    "TMP_DIR = \"fg-data-v8\"\n",
    "DUCKDB_FILE = f\"{TMP_DIR}/taxi.duckdb\"\n",
    "DATA_FOLDER = f\"{TMP_DIR}/taxidata\" \n",
    "\n",
    "# S3 Uploads\n",
    "AWS_ACCESS_KEY=''\n",
    "AWS_SECRET_ACCESS_KEY=''\n",
    "AWS_REGION='us-east-2'\n",
    "BUCKET = \"hopsworks-bench-datasets\"\n",
    "session = boto3.Session(aws_access_key_id=AWS_ACCESS_KEY, aws_secret_access_key=AWS_SECRET_ACCESS_KEY)\n",
    "s3 = session.resource('s3')\n",
    "\n",
    "# HDFS Uploads\n",
    "HOPS_HOST=''\n",
    "HOPS_API_KEY=''\n",
    "HDFS_PATH = \"/Projects/testproj/Resources/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af799cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {TMP_DIR}\n",
    "!mkdir -p {DATA_FOLDER}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4e779b",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect(DUCKDB_FILE, config={'memory_limit': MAX_MEMORY, 'temp_directory': TMP_DIR}) \n",
    "con.execute(\"INSTALL httpfs;\")\n",
    "con.execute(\"INSTALL parquet;\")\n",
    "con.execute(\"LOAD httpfs;\")\n",
    "con.execute(\"LOAD parquet;\")\n",
    "con.execute(f\"\"\"\n",
    "    SET s3_region='{AWS_REGION}';\n",
    "    SET s3_access_key_id='{AWS_ACCESS_KEY}';\n",
    "    SET s3_secret_access_key='{AWS_SECRET_ACCESS_KEY}';\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088c800b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_feature_data(limit, offset):\n",
    "    lim = limit\n",
    "    off = offset\n",
    "    query = f'''\n",
    "        CREATE \n",
    "        OR REPLACE VIEW taxidata \n",
    "        AS\n",
    "        SELECT \n",
    "            tpep_pickup_datetime, \n",
    "            pu_location_id, \n",
    "            pu_borough,\n",
    "            pu_svc_zone,\n",
    "            pu_zone \n",
    "        FROM \n",
    "            read_parquet([\n",
    "                's3://{BUCKET}/taxidata_cleaned/2011.parquet',\n",
    "                's3://{BUCKET}/taxidata_cleaned/2012.parquet',\n",
    "                's3://{BUCKET}/taxidata_cleaned/2013.parquet',\n",
    "                's3://{BUCKET}/taxidata_cleaned/2014.parquet',\n",
    "                's3://{BUCKET}/taxidata_cleaned/2015.parquet',\n",
    "                's3://{BUCKET}/taxidata_cleaned/2016.parquet'\n",
    "            ])\n",
    "    '''\n",
    "    con.execute(query)\n",
    "    raw_data = con.execute(f\"SELECT * FROM taxidata LIMIT {lim} OFFSET {off}\").df()\n",
    "    # Add row_id index to raw_data\n",
    "    raw_data['row_id'] = range(offset, offset + len(raw_data))\n",
    "    row_id = raw_data.pop('row_id')\n",
    "    raw_data.insert(0, 'row_id', row_id)\n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb73cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "project = hopsworks.login()\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454c4707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code needs pandas>=2.0.3\n",
    "from hsfs.core import arrow_flight_client\n",
    "def _get_dataset(descriptor):\n",
    "        info = arrow_flight_client.get_instance()._connection.get_flight_info(descriptor)\n",
    "        reader = arrow_flight_client.get_instance()._connection.do_get(arrow_flight_client.get_instance()._info_to_ticket(info))\n",
    "        table = reader.read_all()\n",
    "        return table.to_pandas(types_mapper=pd.ArrowDtype)\n",
    "arrow_flight_client.get_instance()._get_dataset=_get_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d21f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_factor = [50] # Number of millions of rows to scale \n",
    "\n",
    "for sf in scale_factor:\n",
    "    if sf < 10:\n",
    "        limit = 5000000 # Get 5M at once\n",
    "    elif sf < 20: \n",
    "        limit = 10000000 # Get 10M\n",
    "    elif sf < 50:\n",
    "        limit = 20000000 # Get 20M\n",
    "    else:\n",
    "        limit = 50000000 # Get 50M\n",
    "    offset=0\n",
    "    total_rows = sf * 1000000  # Millions\n",
    "    while offset < total_rows:\n",
    "        print(f\"Total rows: {total_rows}; Offset: {offset}\")\n",
    "        pickup_features = read_feature_data(limit, offset) \n",
    "        pickup_fg = fs.get_or_create_feature_group(\n",
    "            name=f\"pickup_features_{sf}\",\n",
    "            version=1,\n",
    "            primary_key=[\"row_id\"],\n",
    "            event_time=[\"tpep_pickup_datetime\"],\n",
    "            online_enabled=False,\n",
    "            description=\"NYC Taxi data pickup features\")\n",
    "        print(f\"Inserting into FG: pickup_features_{sf}\")\n",
    "        pickup_fg.insert(\n",
    "            pickup_features, \n",
    "            write_options={\n",
    "                \"wait_for_job\" : True,\n",
    "                \"hoodie.deltastreamer.kafka.source.maxEvents\" : limit,\n",
    "            })\n",
    "        offset += limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de63cabb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bae9c1d9",
   "metadata": {},
   "source": [
    "## Benchmark Reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b1abaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_factor = [5, 10, 20, 50]\n",
    "\n",
    "for sf in scale_factor:\n",
    "    pickup_fg = fs.get_feature_group(\n",
    "        name=f\"pickup_features_{sf}\",\n",
    "        version=1\n",
    "    )\n",
    "    start = time.time()\n",
    "    df = pickup_fg.select_all().read()\n",
    "    print(f\"time for SF {sf}: {time.time() - start}\")\n",
    "    print(f\"Num of rows of training data:\\n {df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b86e257",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
